# -*- coding: ascii -*-

from tools import InOut
import nltk
from nltk_contrib.readability.textanalyzer import syllables_en
from nltk.tokenize.punkt import PunktSentenceTokenizer
from nltk.tokenize import RegexpTokenizer
from sentenceprocessor import sentence_processor
import numpy as np
import numpy.random as random
import time
import imp
import sys
from textblob import TextBlob
from textblob_aptagger import PerceptronTagger
import os
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import logging
try:
    import cPickle as pickle 
except:
    import pickle
#=====================================================
base_dir = os.getcwd()
text_dir = "{}/texts".format(base_dir)

list_not_allow = ['SYM', 'TO', '$', "\'\'",
                  '(', ')', ',', '--', '.', ':', 'FW', 'LS', 'UH', "``"]

list_pos = ["CC", "CD", "DT", "EX", "IN", "JJ", "JJR", "JJS", "MD",
            "NN", "NNP", "NNPS", "NNS", "PDT", "POS", "PRP", "PRP$", "RB", "RBR",
            "RBS", "RP", "VB", "VBD", "VBG", "VBN", "VBP", "VBZ","WRB","WDT","WP","WP$"]

log_dir = "{}/logs/{}"

class Sentence_Probability(object):
    """
    This class is for creating the 'training' data. If I want to compare another sentence to the probabilities generated by this
    use the sentence_processor function 
    """
    def __init__(self, filenames, max_line, write_to_file=False, **kwargs):
        """
        Here we assume that the file to be read in is in the "texts" subdirectory
        Also this is a big old shit show. 

        kwargs:
            load_tagged: if True, will load in the previously tagged list of words from a book and the master string.
            load_tot_prob: if True, will load in the total probability array from a tagged book. 
            pos_freq: if provided will calculate the frequency of each part of speech
            in the provided text 
            savefile: filename to be used to save data from this object.
        """
        logging.basicConfig(filename = log_dir.format(base_dir,"log_sentenceprob{}.log".format(time.strftime("%d-%m-%Y"))), level = logging.INFO)
        logging.info('Started: {} {}'.format(time.strftime("%H:%M:%S"), time.strftime("%d/%m/%Y")))
        self.text_dir = text_dir
        """Checking if filenames is a list of strings or just a string"""
        if isinstance(filenames, list):
            self.filenames = list(filenames)
        elif isinstance(filenames, basestring):
            self.filenames = []
            self.filenames.append(filenames)

        if isinstance(max_line, basestring) and max_line == 'max':
            self.max_line = None
        elif isinstance(max_line, int):
            self.max_line = max_line

        self.list_pos = list_pos
        self.savefile = kwargs.get('savefile')
        if self.savefile == None:
            if len(self.filenames) > 1:
                self.savefile = "multi"
            if len(self.filenames) == 1:
                self.savefile = self.filenames[0].strip(".txt")

        load_tot_prob = kwargs.get('load_tot_prob')
        t1 = time.time()
        if load_tot_prob == True:
            probfile = "prob{}.dat".format(self.savefile)
            with InOut(self.text_dir):
                with open(probfile,'r') as loader:
                    self.total_prob = pickle.load(loader)
                    self.up_to_all_probs = self.total_prob.shape[2]
            print("Time loading in total prob: {:.2f} seconds".format(time.time()-t1))
        elif load_tot_prob == False or load_tot_prob == None:
            print("No total prob array loaded.")

        load_tagged = kwargs.get('load_tagged')
        t1 = time.time()
        if load_tagged == True:
            tokenfile = "token1{}.dat".format(self.savefile)
            with InOut(self.text_dir):
                with open(tokenfile,'r') as loader:
                    main_dic = pickle.load(loader)
                    self.blob_tagged_by_sentence = main_dic['var_token']
                    self.sen_tag_pword = main_dic['var_ptoken']
                    self.master_str = main_dic['master_str']
            print("Time loading in tagged lists and master string: {:.2f} seconds".format(time.time()-t1))
            logging.info("Time loading in tagged lists and master string: {:.2f} seconds".format(time.time()-t1))
        elif load_tagged == None or load_tagged == False:
            self.master_str, blob = self.build_master_str(self.filenames)
            self.blob_tagged_by_sentence = [[word[1] for word in sentence.tags if word[1] not in list_not_allow] for sentence in blob.sentences]
            self.sen_tag_pword = [[word for word in sentence.tags if word[1] not in list_not_allow] for sentence in blob.sentences]
            print("Time building tagged lists and master string: {:.2f} seconds".format(time.time()-t1))
            logging.info("Time building tagged lists and master string: {:.2f} seconds".format(time.time()-t1))

        t4 = time.time()
        senttokenizer = PunktSentenceTokenizer()
        wordtokenizer = RegexpTokenizer(r'\w+')
        self.master_sen = senttokenizer.tokenize(self.master_str)
        self.master_word = [word.lower().strip() for word in wordtokenizer.tokenize(self.master_str)]
        self.unique_word = list(set(self.master_word))
        self.num_words = len(self.unique_word)
        self.num_words_total = len(self.master_word)
        self.tknbywrdsent = [[word.lower().strip() for word in wordtokenizer.tokenize(sent)] for sent in self.master_sen]
        print("Time creating tokenized lists: {:.2f} seconds".format(time.time()-t4))

        if write_to_file:
            main_dic = {'var_token':self.blob_tagged_by_sentence,
                        'var_ptoken':self.sen_tag_pword,
                        'master_str':self.master_str}
            print("Writing to file...")
            with InOut(self.text_dir):
                tokenfile = "token1{}.dat".format(self.savefile)
                with open(tokenfile, 'w') as writer:
                    pickle.dump(main_dic,writer)
        else:
            pass
        
        """total_prob is a big 3-D array containing all the combinations of probabilities of parts of 
            speech given all other parts of speech, at each position in the sentence."""


        """Below I create the pos frequency dictionary"""
        pos_freq_kwarg = kwargs.get('pos_freq')

        if pos_freq_kwarg:
            pos_freq = {}
            total_words = 0 
            t3 = time.time()
            for pos in list_pos:
                pos_freq[pos] = 1.0 
            for sentence in self.blob_tagged_by_sentence:
                for pos in sentence:
                    try:
                        pos_freq[pos] += 1.0 
                        total_words += 1.0
                    except KeyError:
                        print("There are part of speech tags in self.blob_tagged_by_sentence that shouldn't be there, namely: "+pos)
            for pos in list_pos:
                pos_freq[pos] = float(pos_freq[pos]/total_words)
            print("Time creating pos frequency dictionary: {:.2f}".format(time.time()-t3))
            self.total_words = total_words
            self.pos_freq = pos_freq
            self.cumu_pos_freq = self.pos_freq
            for i in xrange(1,len(self.list_pos)):
                self.cumu_pos_freq[self.list_pos[i]] += self.cumu_pos_freq[self.list_pos[i-1]]
        elif not pos_freq_kwarg or pos_freq_kwarg == None:
            pass 

    def build_master_str(self,filenames,blob_it=True):
        master_str = str()
        for filename in self.filenames:
            with InOut(text_dir):
                with open(filename, 'r') as reader:
                    for index, line in enumerate(reader):
                        try:
                            line = line.strip('\n').decode('ascii')
                            master_str += line
                            if index == self.max_line:
                                break
                        except UnicodeDecodeError:
                            print("unicode encoding error: Dumping line {} in {}".format(index,filename))
                            continue
        if blob_it:
            t1 = time.time()            
            blob = TextBlob(master_str, pos_tagger=PerceptronTagger())
            print("Time creating object: {:.2f}".format(time.time() - t1))
            logging.info("Time creating object: {:.2f}".format(time.time() - t1))
            return master_str, blob
        elif not blob_it:
            return master_str

    def calc_prob_single(self, wordA, wordB):
        """
        calculates probability of word A following word B, given word B in ANY position. 
        Note also lack of max sentence 
        """
        totalB = 0.0
        totalAgivenB = 0.0
        for sentence in self.tknbywrdsent:
            try:
                index_B = sentence.index(wordB)
                totalB += 1.0
                if sentence[index_B+1] == wordA:
                    totalAgivenB += 1.0

            except (ValueError,IndexError):
                # this means that word B is not in the sentence. or that index_B was the last word in the sentence.
                continue
        try:
            prob = np.float64(totalAgivenB/totalB)
        except ZeroDivisionError:
            prob = 0.0
        return prob


    def graph_prob(self,t_step=None):
        """
        This function is a modified version of the image_plot function above.
        """
        try:
            if t_step != None and t_step > self.up_to_all_probs:
                print("Can't plot for a time step that doesn't exist")
            elif t_step == None:
                plt.ion()
                for i in xrange(self.up_to_all_probs):
                    self.image_plot(self.scale(self.total_prob[:,:,i]))
                    plt.show()
                    raw_input(">> ")
            elif t_step != None and t_step >= 0 and t_step < self.up_to_all_probs:
                self.image_plot(self.scale(self.total_prob[:,:,t_step]))
                plt.show()
        except AttributeError:
            print("You didn't call the all_probs method")

    def image_plot(self,array,**kwargs):
        """
        Automatically scales array you pass to it. 
        """
        try:
            if kwargs["log"]:
                array = self.scale(array,log=True)
            elif kwargs["sqrt"]:
                array = self.scale(array,sqrt=True)
            elif kwargs["exp"]:
                array = self.scale(array,sqrt=True)
        except KeyError:
            array = self.scale(array)
        plot_options = {'cmap':'gray','vmin':0,'vmax':256}
        array = np.asarray(array,dtype=float)
        fig1 = plt.figure(figsize=(16,9))
        ax1 = fig1.add_subplot(111)
        ax1.matshow(array,**plot_options)
        return ax1

    def scale(self,array,**kwargs):
        """
        assuming that the array has no complex numbers
        """
        try:
            if kwargs["log"]:
                array = np.log(np.absolute(array))
            elif kwargs["sqrt"]:
                array = np.sqrt(np.absolute(array))
            elif kwargs["exp"]:
                array = np.exp(array)
        except KeyError:
            pass
        if array.dtype == 'complex':    
            array = np.asarray(array,dtype=complex)
            return 256.*(np.absolute(array))/np.amax(np.fabs(np.absolute(array)))
        else:
            array = np.asarray(array,dtype=float)
            return 256.*(array)/np.amax(array)

    # @classmethod
    def cond_prob_v2(self, magic_range, indexA, indexB, pA, pB):
        """
        this is zero indexed (like Python, chump)
        this function calculates the conditional probability P(A|B) 
        eg "The probability of there being pos "NN" in position 2 given that there is "VB" in position 1."
        tokenized_sentence_list is the list of sentences whose words have been tokenized and p.o.s. tagged.
        indexA is the index of A (in P(A|B)) in the list_pos list
        indexB is the index of B 
        pA is the position of A in the sentence
        pB is the position of B in the sentence 
        pA > pB, or else this is nonsense. (and method won't work)
        magic_range specifies the maximum length of an allowed sentence. 
        """
        # magic_range = magic_range
        tokenized_sentence_list = self.blob_tagged_by_sentence
        total_prob = 0
        cond_prob = 0
        for index1, sentence in enumerate(tokenized_sentence_list):
            if len(sentence)-1 >= pA and len(sentence) <= magic_range: # dont worry about pB, as pA is always greater. 
                if sentence[pB] == list_pos[indexB]:
                    total_prob += 1.0
                    if sentence[pA] == list_pos[indexA]:
                        cond_prob += 1.0
                    else:
                        continue
                else:
                    continue

            else:
                pass
        if cond_prob > total_prob:
            logging.info("conditional probability was higher than total probability. Something isn't working right.")
        try:
            return float(cond_prob / total_prob)
        except ZeroDivisionError:
            return 0.0

    def all_probs(self,up_to=7,write_to_file=False,**kwargs):
        """
        Returns the probability for every A (ie, every part of speech) given every B at each sentence position. 
        There is a len(list_pos) x len(list_pos) array at each position. The indexing is (B,A). 

        note that write to file only actually writes the total_prob variable.

        kwargs:
            max_length: maximum sentence length to use.  
        """
        max_length = kwargs.get("max_length")
        if max_length == None:
            pass
        elif max_length < up_to:
            print("max_length keyword can't be less than up_to. Defaulting to up_to")
            max_length = up_to+1
        self.up_to_all_probs = int(up_to)
        master = np.zeros((len(list_pos),len(list_pos),self.up_to_all_probs),dtype=float)
        masterdict = []
        for h in xrange(0, up_to):
            t1 = time.time()
            A = {}
            for i in xrange(0, len(list_pos)):
                for j in xrange(0, len(list_pos)):
                    prob = self.cond_prob_v2(max_length, j, i, h+1, h) #probability of j given i. This gives (B,A) indexing instead of the other way around.
                    A["{},{}".format(list_pos[j],list_pos[i])] = prob
                    master[i,j,h] = prob
            print("Position {} to {} took {:.1f} sec".format(h,h+1,time.time()-t1))
            logging.info("Position {} to {} took {:.1f} sec".format(h,h+1,time.time()-t1))
            masterdict.append(A)
        self.total_prob = master
        if write_to_file:
            with InOut(self.text_dir):
                probfile = "prob{}.dat".format(self.savefile)
                with open(probfile, 'w') as writer:
                    pickle.dump(self.total_prob,writer)
                    # master_str = str(list(master))
                    # master_str = master_str.replace("array(","")
                    # master_str = master_str.replace(")","")
                    # writer.write("var_list = {}\n".format(master_str))
                    # writer.write("var_dic = {}\n".format(str(masterdict)))
            return {'list':master,'dict':masterdict}
        else:
            return {'list':master,'dict':masterdict}

    def random_word(self,p_o_s):
        """
        p_o_s can either be an integer corresponding to the position in the list_pos list,
        or the name of the part of speech itself.
        """
        if isinstance(p_o_s, int):
            p_o_s = self.list_pos[p_o_s]
        elif isinstance(p_o_s, basestring):
            p_o_s = str(p_o_s)

        start_point = random.randint(0,(len(self.sen_tag_pword)*3)/4)
        for index in xrange(start_point,len(self.sen_tag_pword)-1):
            for word in self.sen_tag_pword[index]:
                if word[1] == p_o_s:
                    return {'word': word[0], 'pos': word[1], 'index': self.list_pos.index(word[1]), 'tupleform':word} 


    def random_word_no_pos(self):
        """
        I need this to be part of the class because I need access to the words in the 
        blob_tagged_by_sentence and sen_tag_pword
        This method just generates a random word. It does it in a weighted manner, however. 
        This means that if p.o.s. 1 occurs more frequently in the text than p.o.s. 2 it 
        will be more likely to choose pos 1. 
        """
        rando = np.random.random()
        for pos in self.list_pos:
            if rando <= self.cumu_pos_freq[pos]:
                special_pos = pos 
                break 
        self.sen_tag_pword = self.sen_tag_pword
        start_point = random.randint(0,(len(tot_sen)*3)/4)
        for index in xrange(start_point,len(tot_sen)-1):
            for word in tot_sen[index]:
                if word[1] == special_pos:
                    return {'word': word[0], 'pos': word[1], 'index': list_pos.index(word[1]),'tupleform':word} 


def sentence_processor(sentence):
    """
    Update 4/2/2015 - Based on the way I've been using this function 
    (just to read in a sentence and tag it) I see no reason to have all 
    the other functionality, like reading in files. 
    """
    master_str = sentence
    t1 = time.time()
    blob = TextBlob(master_str, pos_tagger=PerceptronTagger())
    tagged_by_sen = [[word for word in sentence.tags if word[1] not in list_not_allow] for sentence in blob.sentences]
    print("Time loading in sentence {:.2f}".format(time.time() - t1))
    return tagged_by_sen




